{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Password Strength Estimation Model (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets tokenizers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import yaml\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from zxcvbn import zxcvbn\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, RobertaTokenizerFast, TrainingArguments, Trainer\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling, _torch_collate_batch\n",
    "from tokenizers import ByteLevelBPETokenizer, trainers\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset, disable_caching\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom Tokenizer\n",
    "\n",
    "Implement a character-level tokenizer specifically for passwords. This approach ensures each character is treated as a separate token, unlike typical NLP tokenizers that group letters into words. This method maintains a more accurate probability distribution for password modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassTokenizer(ByteLevelBPETokenizer):\n",
    "    \"\"\"ByteLevelBPETokenizer\n",
    "    Represents a Byte-level BPE as introduced by OpenAI with their GPT-2 model\n",
    "    \"\"\"\n",
    "\n",
    "    def train_from_iterator(\n",
    "        self,\n",
    "        iterator,\n",
    "        vocab_size: int = 30000,\n",
    "        min_frequency: int = 2,\n",
    "        show_progress: bool = True,\n",
    "        special_tokens = [],\n",
    "        length = None,\n",
    "    ):\n",
    "        \"\"\"Train the model using the given iterator\"\"\"\n",
    "\n",
    "        trainer = trainers.BpeTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            show_progress=show_progress,\n",
    "            special_tokens=special_tokens,\n",
    "            initial_alphabet=[],\n",
    "        )\n",
    "        self._tokenizer.train_from_iterator(\n",
    "            iterator,\n",
    "            trainer=trainer,\n",
    "            length=length,\n",
    "        )\n",
    "\n",
    "# Set the paths directly\n",
    "train_path = \"../bruteforce-database/uniqpass-v16-passwords.txt\"  # Replace with your actual training dataset path\n",
    "output_path = \"./uniqpass-v16-passwords-tokenizer/\"  # Replace with your actual output directory path\n",
    "\n",
    "print(\"ü§û Reading passwords\")\n",
    "\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "# Filter printable passwords\n",
    "ascii_printable = []\n",
    "for p in lines:\n",
    "    if all(32 < ord(c) < 128 for c in p):\n",
    "        ascii_printable.append(p)\n",
    "    \n",
    "# Log information about your data\n",
    "all_chars = ''.join(ascii_printable)  # concatenate all strings into a single string\n",
    "unique_chars = set(all_chars)\n",
    "count = len(unique_chars)\n",
    "print(f\"The number of distinct letters in all strings is {count}\")\n",
    "\n",
    "# Customize training\n",
    "special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\", # This will be used to indicate end of password\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "]\n",
    "\n",
    "# Create BPE tokenizer\n",
    "print(\"ü§û Training tokenizer\")\n",
    "tokenizer = PassTokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train_from_iterator(ascii_printable, vocab_size=count+len(special_tokens), min_frequency=1, special_tokens=special_tokens)\n",
    "\n",
    "print(\"ü§û Tokenizer trained with vocabulary\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(sorted(vocab, key=lambda x: vocab[x]))\n",
    "\n",
    "output_dir = Path(output_path)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "   \n",
    "# Export\n",
    "tokenizer.save_model(str(output_dir))  # Convert Path object to string\n",
    "print(\"‚úÖ Tokenizer exported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-process\n",
    "\n",
    "Some of the data preprocessing steps that can be done are:\n",
    "1. Remove passwords with length smaller than 8\n",
    "2. Remove passwords with length greater than 64\n",
    "3. Remove passwords with non-ASCII characters\n",
    "4. Remove passwords with non-printable characters\n",
    "5. Remove passwords with non-alphanumeric characters\n",
    "6. Remove passwords with repeating characters\n",
    "7. Remove passwords with repeating patterns\n",
    "8. Remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load uniqpass-v16-passwords and only extract printable passwords over 10 characters\n",
    "print(\"ü§û Reading passwords\")\n",
    "with open(\"../bruteforce-database/uniqpass-v16-passwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    \n",
    "# Filter printable passwords\n",
    "ascii_printable = []\n",
    "for p in lines:\n",
    "    if all(32 < ord(c) < 128 for c in p):\n",
    "        ascii_printable.append(p)\n",
    "        \n",
    "# Filter passwords over 10 characters\n",
    "ascii_printable = [p for p in ascii_printable if len(p) < 16]\n",
    "\n",
    "# Get length of of ascii_printable array\n",
    "count = len(ascii_printable)\n",
    "print(f\"The number of passwords in ascii_printable is {count}\")\n",
    "\n",
    "myset = set(ascii_printable)\n",
    "setcount = len(myset)\n",
    "print(f\"The number of passwords in myset is {setcount}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader and Configuration\n",
    "\n",
    "Password lists are generally line separated and can be loaded using our custom data `PasswordDataCollator` to help facilitate the embedding of the data.\n",
    "\n",
    "Configuration can be loaded in via the `config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "# Details for model architecture. Set parameters directly for GPT2Config (https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Config)\n",
    "model_args:\n",
    "    n_head: 12 # Number of attention heads for each attention layer in the Transformer encoder.\n",
    "    n_layer: 8 # Number of transformer layers\n",
    "\n",
    "# Execution-wide parameters\n",
    "config_args:\n",
    "    seed: 14\n",
    "    maxchars: 16 # Maximum characters to be considered in your passwords\n",
    "    subsample: -1 # -1 means no subsampling training data\n",
    "    tokenizer_path: './uniqpass-v16-passwords-tokenizer' # Introdue the path or huggingface name for your tokenizer\n",
    "    train_data_path: '../bruteforce-database/uniqpass-v16-passwords.txt' # Path to your training data\n",
    "\n",
    "# Set parameters directly for TrainingArguments (https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)\n",
    "training_args:\n",
    "    per_device_train_batch_size: 512 # Batch size per GPU/CPU for training\n",
    "    gradient_accumulation_steps: 4 # Number of updates steps to accumulate before performing a backward/update pass.\n",
    "    logging_steps: 50 # Number of steps between logging\n",
    "    save_total_limit: 1 # Limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    num_train_epochs: 1 # Number of training epochs\n",
    "    overwrite_output_dir: true # Overwrite the content of the output directory\n",
    "    fp16: false # Whether to use 16-bit (mixed) precision training instead of 32-bit training (Does not work on MPS devices)\n",
    "    output_dir: './uniqpass-v16-passwords-trained' # Where to store your checkpoints\n",
    "    report_to: \"wandb\" # options are \"wandb\", \"tensorboard\", \"mlflow\", \"none\"\n",
    "    save_steps: 200  # Number of updates steps before two checkpoint saves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PasswordDataCollator(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    CustomDataCollator for this task. It modifies the special token mask so that the end of password token is not ignored (should also be predicted).\n",
    "    \"\"\"\n",
    "    def torch_call(self, examples):\n",
    "        # Handle dict or lists with proper padding and conversion to tensor.\n",
    "        if isinstance(examples[0], (dict, BatchEncoding)):\n",
    "            batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        else:\n",
    "            batch = {\n",
    "                \"input_ids\": _torch_collate_batch(examples, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "            }\n",
    "\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None) # Remove if given\n",
    "        \n",
    "        # Create custom special tokens mask\n",
    "        special_tokens_mask = torch.where(batch['input_ids'] != self.tokenizer.pad_token_id, 0, 1)\n",
    "        \n",
    "        if self.mlm:\n",
    "            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
    "                batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "            )\n",
    "        else:\n",
    "            labels = batch[\"input_ids\"].clone()\n",
    "            if self.tokenizer.pad_token_id is not None:\n",
    "                labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "            batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "    \n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "# Load config from file\n",
    "config_path = \"./config.yaml\"  # Replace with your actual config path\n",
    "\n",
    "# Check if MPS is available and set it as the default device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # Fallback to CPU if MPS is not available\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "args = dotdict(config[\"config_args\"])\n",
    "model_args = dotdict(config[\"model_args\"])\n",
    "training_args = dotdict(config[\"training_args\"])\n",
    "training_args[\"seed\"] = args.seed\n",
    "\n",
    "# Init random seeds\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "assert not os.path.exists(training_args.output_dir), \"The provided output path already exists, please provide a unique path.\"\n",
    "Path(training_args.output_dir).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer the Data\n",
    "\n",
    "Here we will use the custom tokenizer to tokenize the data into our training testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Declare constants\n",
    "TOKENIZER_MAX_LEN = args.maxchars + 2 # Additional characters for start and end of password tokens\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"ü§û Loading tokenizer\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(args.tokenizer_path, \n",
    "                                                  max_len=TOKENIZER_MAX_LEN,\n",
    "                                                  padding=\"max_length\", \n",
    "                                                  truncation=True,\n",
    "                                                  do_lower_case=False,\n",
    "                                                  strip_accents=False,\n",
    "                                                  mask_token=\"<mask>\",\n",
    "                                                  unk_token=\"<unk>\",\n",
    "                                                  pad_token=\"<pad>\",\n",
    "                                                  truncation_side=\"right\")\n",
    "\n",
    "# Define dataloader\n",
    "print(\"ü§û Loading data\")\n",
    "\n",
    "def preprocess_function(entries):\n",
    "    \"\"\"\n",
    "    This function tokenizes a list of passwords. It appends the end of password token to each of them before processing.\n",
    "    \"\"\"\n",
    "    to_tokenize = ['<s>' + p[:args.maxchars] +'</s>' for p in entries['text']]\n",
    "    return tokenizer(to_tokenize, \n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\", \n",
    "                     max_length=TOKENIZER_MAX_LEN, \n",
    "                     add_special_tokens=False, \n",
    "                     return_special_tokens_mask=False)\n",
    "\n",
    "data_files = {'train': [args.train_data_path]}\n",
    "dataset = load_dataset('text', data_files=data_files)\n",
    "print(\"Dataset loaded with {} entries\".format(len(dataset[\"train\"])))\n",
    "\n",
    "if args.subsample > 0:\n",
    "    print(\"Subsampling dataset to {} random entries\".format(args.subsample))\n",
    "    dataset['train'] = dataset['train'].select([i for i in range(args.subsample)])\n",
    "    \n",
    "# Process data\n",
    "print(\"ü§û Processing data\")\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_datasets = tokenized_datasets.shuffle(seed=args.seed)\n",
    "\n",
    "# Format data\n",
    "tokenized_datasets.set_format(type=\"torch\")\n",
    "\n",
    "print(\"ü§û Initializing model\")\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    **model_args\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "print(\"Model initialized with {} parameters\".format(sum(t.numel() for t in model.parameters())))\n",
    "\n",
    "print(\"ü§û Preparing training\")\n",
    "# Define the data collator. In charge of hiding tokens to be predicted.\n",
    "data_collator = PasswordDataCollator(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "train_args = TrainingArguments(**training_args)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"]\n",
    ")\n",
    "\n",
    "print(\"üöÄ Launching training\")\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "print(\"üëç Training completed after {}. Storing last version.\".format(str(timedelta(seconds=end-start))))\n",
    "model.save_pretrained(os.path.join(training_args.output_dir, \"last\"))\n",
    "\n",
    "# Comment out next lines if you want to keep several checkpoints.\n",
    "print(\"üóëÔ∏è Deleting previous checkpoints\")\n",
    "checkpoints = [i for i in os.listdir(training_args.output_dir) if i.startswith(\"checkpoint\")]\n",
    "for c in checkpoints: \n",
    "    shutil.rmtree(os.path.join(training_args.output_dir, c))\n",
    "\n",
    "print(\"‚úÖ Training finished successfully :)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test The Model\n",
    "\n",
    "This model can now be used for single or conditional generation of passwords. The model can be used to generate passwords from scratch or to generate passwords based on a prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Generation of Password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, RobertaTokenizerFast\n",
    "\n",
    "NUM_GENERATIONS = 1\n",
    "MAX_CHARS = 10\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./uniqpass-v16-passwords-tokenizer/\",\n",
    "                                                  max_len=MAX_CHARS + 2, # Max length + start and end tokens\n",
    "                                                  padding=\"max_length\", \n",
    "                                                  truncation=True,\n",
    "                                                  do_lower_case=False,\n",
    "                                                  strip_accents=False,\n",
    "                                                  mask_token=\"<mask>\",\n",
    "                                                  unk_token=\"<unk>\",\n",
    "                                                  pad_token=\"<pad>\",\n",
    "                                                  truncation_side=\"right\")\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./uniqpass-v16-passwords-trained/last\").eval()\n",
    "\n",
    "# Generate passwords sampling from the beginning of password token\n",
    "g = model.generate(torch.tensor([[tokenizer.bos_token_id]]),\n",
    "                  do_sample=True,\n",
    "                  num_return_sequences=NUM_GENERATIONS,\n",
    "                  max_length=MAX_CHARS+2, # Max length + start and end tokens\n",
    "                  pad_token_id=tokenizer.pad_token_id,\n",
    "                  bad_words_ids=[[tokenizer.bos_token_id]])\n",
    "\n",
    "# Remove start of sentence token\n",
    "g = g[:, 1:]\n",
    "\n",
    "decoded = tokenizer.batch_decode(g.tolist())\n",
    "decoded_clean = [i.split(\"</s>\")[0] for i in decoded] # Get content before end of password token\n",
    "\n",
    "# Print your sampled password\n",
    "print(decoded_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Generation of Passwords\n",
    "\n",
    "Here we can generate multiple passwords at once. This can be used to generate a large number of passwords for a password list for log evaluation to determine the strength of the password generation abilities of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, RobertaTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set your parameters here\n",
    "model_path = \"./uniqpass-v16-passwords-trained/last\"  # Replace with your model path\n",
    "tokenizer_path = \"./uniqpass-v16-passwords-tokenizer/\"  # Assuming tokenizer is saved at model path\n",
    "train_data_path = \"../bruteforce-database/uniqpass-v16-passwords.txt\"  # Replace with your training data path\n",
    "eval_data_path = \"../bruteforce-database/38650-password-sktorrent.txt\"  # Replace with your evaluation data path\n",
    "out_path = \"./generated_passwords_uniqpass-v16-passwords_trained_1000000/\"  # Replace with your output path\n",
    "filename = \"passwords.txt\"\n",
    "maxchars = 16  # Set your max characters\n",
    "num_generate = 1000000  # Number of passwords to generate\n",
    "batch_size = 512  # Batch size for generation\n",
    "num_beams = 1\n",
    "top_p = 95\n",
    "top_k = None\n",
    "temperature = 1.2\n",
    "seed_offset = 42  # Seed for randomness\n",
    "\n",
    "# Set device to MPS if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "# Init random seeds\n",
    "random.seed(seed_offset)\n",
    "np.random.seed(seed_offset)\n",
    "torch.manual_seed(seed_offset)\n",
    "\n",
    "# Ensure output path exists\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "assert not os.path.isfile(os.path.join(out_path, filename)), \"The provided output path already exists, please provide a unique path.\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path, \n",
    "                                                 max_len=maxchars+2,\n",
    "                                                 padding=\"max_length\", \n",
    "                                                 truncation=True,\n",
    "                                                 do_lower_case=False,\n",
    "                                                 strip_accents=False,\n",
    "                                                 mask_token=\"<mask>\",\n",
    "                                                 unk_token=\"<unk>\",\n",
    "                                                 pad_token=\"<pad>\",\n",
    "                                                 truncation_side=\"right\")\n",
    "\n",
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).eval().to(device)\n",
    "\n",
    "# Passwords generation\n",
    "generations = []\n",
    "\n",
    "for i in tqdm(range(int(num_generate / batch_size)), desc=\"Generating passwords\"):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(seed_offset + i)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate tokens sampling from the distribution of codebook indices\n",
    "        g = model.generate(torch.tensor([[tokenizer.bos_token_id]]).to(device), \n",
    "                           do_sample=True, \n",
    "                           max_length=maxchars+2, \n",
    "                           pad_token_id=tokenizer.pad_token_id, \n",
    "                           bad_words_ids=[[tokenizer.bos_token_id]], \n",
    "                           num_return_sequences=batch_size, \n",
    "                           num_beams=num_beams, \n",
    "                           top_p=top_p / 100, \n",
    "                           top_k=top_k, \n",
    "                           temperature=temperature)\n",
    "\n",
    "        # Remove start of sentence token\n",
    "        g = g[:, 1:]\n",
    "\n",
    "    decoded = tokenizer.batch_decode(g.tolist())\n",
    "    decoded_clean = [i.split(\"</s>\")[0] for i in decoded]  # Get content before end of password token\n",
    "\n",
    "    generations += decoded_clean\n",
    "\n",
    "# Store passwords\n",
    "with open(os.path.join(out_path, filename), 'w') as f:\n",
    "    for line in generations:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "# Log information\n",
    "num_generated = len(generations)\n",
    "num_unique = len(set(generations))\n",
    "perc_unique = num_unique / num_generated * 100\n",
    "\n",
    "data_files = {}\n",
    "\n",
    "if train_data_path:\n",
    "    data_files[\"train\"] = [train_data_path]\n",
    "\n",
    "if eval_data_path:\n",
    "    data_files[\"eval\"] = [eval_data_path]\n",
    "\n",
    "if data_files:\n",
    "    dataset = load_dataset('text', data_files=data_files, encoding=\"latin-1\")\n",
    "\n",
    "    if train_data_path:\n",
    "        train_passwords = set(dataset[\"train\"][\"text\"])\n",
    "        inter_with_train = len(train_passwords.intersection(set(generations)))\n",
    "\n",
    "    if eval_data_path:\n",
    "        eval_passwords = set(dataset[\"eval\"][\"text\"])\n",
    "        inter_with_eval = len(eval_passwords.intersection(set(generations)))\n",
    "\n",
    "# Log details\n",
    "with open(os.path.join(out_path, f\"log_{filename}\"), 'w') as f:\n",
    "    f.write(f\"Passwords generated using model at: {model_path}\\n\")\n",
    "    f.write(f\"Number of passwords generated: {num_generated}\\n\")\n",
    "    f.write(f\"{num_unique} unique passwords generated => {perc_unique:.2f}%\\n\")\n",
    "    if train_data_path:\n",
    "        f.write(f\"{inter_with_train} passwords were found in the training set. {100 * inter_with_train / len(train_passwords):.5f}% of the train set guessed.\\n\")\n",
    "    if eval_data_path:\n",
    "        f.write(f\"{inter_with_eval} passwords were found in the test set. {100 * inter_with_eval / len(eval_passwords):.5f}% of the test set guessed.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results of batching process over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10^3 - 10^6\n",
    "number_of_guesses = [1000, 10000, 100000, 1000000]  # Number of guesses\n",
    "\n",
    "# Extracted from the log files\n",
    "unique_passwords_percent = [100.00, 100.00, 99.85, 98.95]  # Corresponding percentage of unique passwords\n",
    "\n",
    "# Create Chart to Show Percentage of Unique Passwords vs. Number of Guesses\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the data with a label for the legend\n",
    "plt.plot(number_of_guesses, unique_passwords_percent, marker='o', linestyle='-', color='blue', label='GPTCracker')\n",
    "\n",
    "# Setting the x-axis to logarithmic scale\n",
    "plt.xscale('log')\n",
    "\n",
    "# Set the x-axis limits to show from 10^4 to 10^9\n",
    "plt.xlim(10**4, 10**6)\n",
    "\n",
    "# Set the y-axis limits from lowest value to 100\n",
    "plt.yticks(np.arange(min(unique_passwords_percent) -5, 100, 1))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number of Guesses (Log Scale)')\n",
    "plt.ylabel('Percentage of Unique Passwords (%)')\n",
    "plt.title('Percentage of Unique Passwords vs. Number of Guesses')\n",
    "\n",
    "# Adding the legend to the top right\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot with a grid\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Password Strength Classification\n",
    "\n",
    "Here we can utilize the model to classify the strength of a password by extracting the log likelihood and determine if it is high or low to numerically understand the strength of the password. \n",
    "\n",
    "We can then compare the strength of the password through open-source tools like `zxcvbn` to determine if the model is generating strong passwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./uniqpass-v16-passwords-trained/last\").eval()\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./uniqpass-v16-passwords-tokenizer/\")\n",
    "\n",
    "def calculate_password_probability(model, tokenizer, password):\n",
    "    # Tokenize the password and convert to tensor format\n",
    "    input_ids = tokenizer.encode(password, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get the model's logit predictions for the password\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Calculate the probability of the password sequence\n",
    "    # We take the product of probabilities for the actual tokens in input_ids\n",
    "    # The first token is the BOS token, which we skip\n",
    "    input_id_probs = probabilities[:, :-1].gather(2, input_ids[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    password_probability = torch.prod(input_id_probs).item()\n",
    "    \n",
    "    return password_probability\n",
    "\n",
    "\n",
    "def assess_password_strength(password, model, tokenizer):\n",
    "    # Get password strength estimation from zxcvbn\n",
    "    # 0 means the password is very weak (might be cracked in a few guesses)\n",
    "    # 1 means the password is weak (might be cracked in minutes)\n",
    "    # 2 means the password is fair (might be cracked in hours)\n",
    "    # 3 means the password is strong (might be cracked in days)\n",
    "    # 4 means the password is very strong (might be cracked in centuries)\n",
    "    zxcvbn_strength = zxcvbn(password)['score']\n",
    "    \n",
    "    # Get password probability from model\n",
    "    password_probability = calculate_password_probability(model, tokenizer, password)\n",
    "    \n",
    "    # Combine the two metrics to assess password strength\n",
    "    combined_strength = zxcvbn_strength * (1 - password_probability)\n",
    "    \n",
    "    return combined_strength, password_probability, zxcvbn_strength\n",
    "\n",
    "\n",
    "# Load passwords from ./generated_passwords/passwords.txt\n",
    "with open(\"./generated_passwords_uniqpass-v16-passwords_trained_10000/passwords.txt\", \"r\") as f:\n",
    "    passwords = f.read().splitlines()\n",
    "    \n",
    "# passwords = passwords + [\"123456\", \"123456789\", \"qwerty\", \"password\", \"12345\", \"qwerty123\", \"1q2w3e\", \"12345678\", \"111111\", \"1234567890\"]    \n",
    "\n",
    "# Loop over passwords and get strength\n",
    "for i, password in enumerate(passwords):\n",
    "    combined_strength, password_probability, zxcvbn_strength = assess_password_strength(password, model, tokenizer)\n",
    "    print(f\"Combined strength score for password '{password}': {combined_strength} - Prob: {password_probability}, Zxcvbn: {zxcvbn_strength}\")\n",
    "    \n",
    "    # If first index add column names first: password,combined_strength,password_probability,zxcvbn_strength otherwie just add new record\n",
    "    if i == 0:\n",
    "        with open(\"./generated_passwords_uniqpass-v16-passwords_trained_1000/passwords_strength.csv\", \"a\") as f:\n",
    "            f.write(f\"password,combined_strength,password_probability,zxcvbn_strength\\n\")\n",
    "            f.write(f\"{password},{combined_strength},{password_probability},{zxcvbn_strength}\\n\")\n",
    "    else:\n",
    "        with open(\"./generated_passwords_uniqpass-v16-passwords_trained_1000/passwords_strength.csv\", \"a\") as f:\n",
    "            f.write(f\"{password},{combined_strength},{password_probability},{zxcvbn_strength}\\n\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
